{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langgraph==0.3.2\n",
      "langmem==0.0.14\n",
      "sentence-transformers==3.4.1\n",
      "torch==2.6.0\n",
      "numpy==2.2.3\n",
      "openai==1.65.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkottala\\AppData\\Local\\Temp\\ipykernel_18520\\356668026.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "packages = [\"langgraph\", \"langmem\", \"sentence-transformers\", \"torch\", \"numpy\", \"openai\"]\n",
    "\n",
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    for pkg in packages:\n",
    "        try:\n",
    "            version = pkg_resources.get_distribution(pkg).version\n",
    "            line = f\"{pkg}=={version}\\n\"\n",
    "            print(line.strip())  # Print to console\n",
    "            f.write(line)  # Write to file\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            print(f\"{pkg} is not installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jkottala\\Envs\\langGraph\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (768,)\n",
      "First few values: [-0.01384925  0.06499632 -0.03022257 -0.03865709  0.02064267]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model (downloads ~420MB first time)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Test it\n",
    "text = \"I like Python\"\n",
    "embedding = model.encode(text)\n",
    "print(f\"Embedding shape: {embedding.shape}\")  # Should be (768,)\n",
    "print(f\"First few values: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 13:26:48,156 - INFO - Client ID and Client Secret found [azure_openai_llm.py:54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It seems you might be mistaking me for Azure, but I’m here to assist you with any questions or information you need. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from azure_openai_llm import get_llm\n",
    "\n",
    "llm = get_llm()\n",
    "\n",
    "res = llm.invoke(input=[{\"role\": \"user\", \"content\": \"Hello, Azure!\"}])\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jkottala\\Envs\\langGraph\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-04 16:17:06,061 - INFO - Client ID and Client Secret found [azure_openai_llm.py:54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemBot: Hi! Ask me anything. (Type 'exit' to stop)\n",
      "MemBot: Hello! How can I assist you today?\n",
      "\n",
      "--- Stored Memories ---\n",
      "No memories stored yet.\n",
      "----------------------\n",
      "\n",
      "MemBot: I've noted that you like Python! If you have any questions or topics related to Python that you'd like to discuss, feel free to ask!\n",
      "\n",
      "--- Stored Memories ---\n",
      "Error occurred:  BaseStore.get() missing 1 required positional argument: 'key'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from azure_openai_llm import get_llm\n",
    "\n",
    "# Load local embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# Embedding function for LangMem\n",
    "def embed_func(text):\n",
    "    return model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "# Setup memory store\n",
    "store = InMemoryStore(index={\"dims\": 384, \"embed\": embed_func})\n",
    "\n",
    "# Get Azure ChatGPT model\n",
    "llm = get_llm()\n",
    "\n",
    "# Create memory tools with namespace\n",
    "manage_memory = create_manage_memory_tool(namespace=(\"user_1\",))\n",
    "search_memory = create_search_memory_tool(namespace=(\"user_1\",))\n",
    "\n",
    "# Create agent with memory tools\n",
    "agent = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=[manage_memory, search_memory],\n",
    "    store=store\n",
    ")\n",
    "\n",
    "def print_stored_memories():\n",
    "    print(\"\\n--- Stored Memories ---\")\n",
    "\n",
    "    # Check if the store has stored data\n",
    "    if hasattr(store, \"_data\") and isinstance(store._data, dict):\n",
    "        all_memories = list(store._data.keys())  # Get stored keys\n",
    "    else:\n",
    "        print(\"Error: Cannot retrieve stored keys.\")\n",
    "        return\n",
    "\n",
    "    if not all_memories:\n",
    "        print(\"No memories stored yet.\")\n",
    "    else:\n",
    "        # Fetch values without extra namespace argument\n",
    "        values = [store.get(key) for key in all_memories]\n",
    "\n",
    "        for i, (key, value) in enumerate(zip(all_memories, values)):\n",
    "            print(f\"Memory {i+1}: Key={key}, Value={value}\")\n",
    "\n",
    "    print(\"----------------------\\n\")\n",
    "\n",
    "# Chat loop with memory\n",
    "def chat_with_membot():\n",
    "    print(\"MemBot: Hi! Ask me anything. (Type 'exit' to stop)\")\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"MemBot: Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Run the agent with memory\n",
    "            response = agent.invoke({'messages': [{\"role\": \"user\", \"content\": user_input}]})\n",
    "            \n",
    "            # Extract AI response (FIXED)\n",
    "            if isinstance(response, dict) and \"messages\" in response:\n",
    "                ai_response = response[\"messages\"][-1].content  # FIXED\n",
    "            else:\n",
    "                ai_response = str(response)\n",
    "            \n",
    "            print(f\"MemBot: {ai_response}\")\n",
    "            \n",
    "            # Print what’s stored after each turn\n",
    "            print_stored_memories()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred: \", e)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"MemBot: Goodbye!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_membot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Prompt', 'ReflectionExecutor', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'create_manage_memory_tool', 'create_memory_manager', 'create_memory_searcher', 'create_memory_store_manager', 'create_multi_prompt_optimizer', 'create_prompt_optimizer', 'create_search_memory_tool', 'create_thread_extractor', 'errors', 'knowledge', 'prompts', 'reflection', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import langmem\n",
    "print(dir(langmem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jkottala\\Envs\\langGraph\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-05 07:58:56,510 - INFO - Client ID and Client Secret found [azure_openai_llm.py:54]\n"
     ]
    }
   ],
   "source": [
    "# In the Hot Path\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.utils.config import get_store \n",
    "from langmem import (\n",
    "    # Lets agent create, update, and delete memories \n",
    "    create_manage_memory_tool,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from azure_openai_llm import get_llm\n",
    "from uuid import uuid4\n",
    "\n",
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "def embed_text(text: str) -> list:\n",
    "    \"\"\"Convert text to embeddings using all-MiniLM-L12-v2.\"\"\"\n",
    "    return embedding_model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "store = InMemoryStore(index={\"dims\": 384, \"embed\": embed_text}) \n",
    "\n",
    "def prompt(state):\n",
    "    \"\"\"Prepare the messages for the LLM.\"\"\"\n",
    "    # Get store from configured contextvar; \n",
    "    store = get_store() # Same as that provided to `create_react_agent`\n",
    "    memories = store.search(\n",
    "        # Search within the same namespace as the one\n",
    "        # we've configured for the agent\n",
    "        (\"memories\",),\n",
    "        query=state[\"messages\"][-1].content,\n",
    "    )\n",
    "    system_msg = f\"\"\"You are a helpful assistant.\n",
    "\n",
    "## Memories\n",
    "<memories>\n",
    "{memories}\n",
    "</memories>\n",
    "\"\"\"\n",
    "    return [{\"role\": \"system\", \"content\": system_msg}, *state[\"messages\"]]\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver() # Checkpoint graph state \n",
    "\n",
    "agent = create_react_agent( \n",
    "    model=get_llm(),\n",
    "    prompt=prompt,\n",
    "    tools=[ # Add memory tools \n",
    "        # The agent can call \"manage_memory\" to\n",
    "        # create, update, and delete memories by ID\n",
    "        # Namespaces add scope to memories. To\n",
    "        # scope memories per-user, do (\"memories\", \"{user_id}\"): \n",
    "        create_manage_memory_tool(namespace=(\"memories\",)),\n",
    "    ],\n",
    "    # Our memories will be stored in this provided BaseStore instance\n",
    "    store=store,\n",
    "    # And the graph \"state\" will be checkpointed after each node\n",
    "    # completes executing for tracking the chat history and durable execution\n",
    "    checkpointer=checkpointer, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I currently don't have any information about your preferred display mode. Would you like me to remember your preference now? If so, please let me know what it is!\n",
      "I currently don't have any information about your preferred display mode. Would you like me to remember your preference now? If so, please let me know what it is!\n",
      "Yes, I remember you! You prefer a dark display mode. If you have any other preferences or requests, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"thread-a\"}}\n",
    "\n",
    "# Use the agent. The agent hasn't saved any memories,\n",
    "# so it doesn't know about us\n",
    "response = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Know which display mode I prefer?\"}\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "# Output: \"I don't seem to have any stored memories about your display mode preferences...\"\n",
    "\n",
    "agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"dark. Remember that.\"}\n",
    "        ]\n",
    "    },\n",
    "    # We will continue the conversation (thread-a) by using the config with\n",
    "    # the same thread_id\n",
    "    config=config,\n",
    ")\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "# New thread = new conversation!\n",
    "new_config = {\"configurable\": {\"thread_id\": \"thread-b\"}}\n",
    "# The agent will only be able to recall\n",
    "# whatever it explicitly saved using the manage_memories tool\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hey there. Do you remember me? What are my preferences?\"}]},\n",
    "    config=new_config,\n",
    ")\n",
    "print(response[\"messages\"][-1].content)\n",
    "# Output: \"Based on my memory search, I can see that you've previously indicated a preference for dark display mode...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I’m based on OpenAI's GPT-3 model. If you have any specific questions or need assistance, feel free to ask!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 27, 'total_tokens': 54, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b705f0c291', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run-78f9ee88-714b-4bf3-85fe-faa620f5f371-0' usage_metadata={'input_tokens': 27, 'output_tokens': 27, 'total_tokens': 54, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What is your model name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 08:10:37,931 - INFO - Client ID and Client Secret found [azure_openai_llm.py:54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! Dogs can be wonderful companions. What kind of breed is Fido, and what do you enjoy doing together?\n"
     ]
    }
   ],
   "source": [
    "# In the Background\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.func import entrypoint\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from langmem import ReflectionExecutor, create_memory_store_manager\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from azure_openai_llm import get_llm\n",
    "from uuid import uuid4\n",
    "\n",
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "def embed_text(text: str) -> list:\n",
    "    \"\"\"Convert text to embeddings using all-MiniLM-L12-v2.\"\"\"\n",
    "    return embedding_model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "store = InMemoryStore(index={\"dims\": 384, \"embed\": embed_text})\n",
    "\n",
    "llm = get_llm()\n",
    "\n",
    "# Create memory manager Runnable to extract memories from conversations\n",
    "memory_manager = create_memory_store_manager(\n",
    "    llm,\n",
    "    # Store memories in the \"memories\" namespace (aka directory)\n",
    "    namespace=(\"memories\",),  # \n",
    ")\n",
    "\n",
    "@entrypoint(store=store)  # Create a LangGraph workflow\n",
    "async def chat(message: str):\n",
    "    response = llm.invoke(message)\n",
    "\n",
    "    # memory_manager extracts memories from conversation history\n",
    "    # We'll provide it in OpenAI's message format\n",
    "    to_process = {\"messages\": [{\"role\": \"user\", \"content\": message}] + [response]}\n",
    "    await memory_manager.ainvoke(to_process)  # \n",
    "    return response.content\n",
    "# Run conversation as normal\n",
    "response = await chat.ainvoke(\n",
    "    \"I like dogs. My dog's name is Fido.\",\n",
    ")\n",
    "print(response)\n",
    "# Output: That's nice! Dogs make wonderful companions. Fido is a classic dog name. What kind of dog is Fido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Item(namespace=['memories'], key='cc0f7be7-0fa6-4b05-8d92-4e829916e024', value={'kind': 'Memory', 'content': {'content': 'The user likes dogs and has a dog named Fido.'}}, created_at='2025-03-05T02:40:49.796761+00:00', updated_at='2025-03-05T02:40:49.796761+00:00', score=None)]\n"
     ]
    }
   ],
   "source": [
    "print(store.search((\"memories\",)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 08:52:02,015 - INFO - Client ID and Client Secret found [azure_openai_llm.py:54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! Dogs make wonderful companions. How long have you had Fido? What breed is he?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.func import entrypoint\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from langmem import ReflectionExecutor, create_memory_store_manager\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from azure_openai_llm import get_llm\n",
    "\n",
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "def embed_text(text: str) -> list:\n",
    "    \"\"\"Convert text to embeddings using all-MiniLM-L12-v2.\"\"\"\n",
    "    return embedding_model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "store = InMemoryStore(index={\"dims\": 384, \"embed\": embed_text})\n",
    "\n",
    "llm = get_llm()\n",
    "\n",
    "# Create memory manager Runnable to extract memories from conversations\n",
    "memory_manager = create_memory_store_manager(\n",
    "    llm,\n",
    "    namespace=(\"memories\",),\n",
    ")\n",
    "\n",
    "async def extract_memories_async(to_process):\n",
    "    \"\"\"Run memory extraction in the background.\"\"\"\n",
    "    loop = asyncio.get_running_loop()\n",
    "    loop.create_task(memory_manager.ainvoke(to_process))\n",
    "\n",
    "@entrypoint(store=store)\n",
    "async def chat(message: str):\n",
    "    response = llm.invoke(message)\n",
    "\n",
    "    # Prepare conversation history for memory extraction\n",
    "    to_process = {\"messages\": [{\"role\": \"user\", \"content\": message}] + [response]}\n",
    "    \n",
    "    # Trigger background memory extraction\n",
    "    asyncio.create_task(extract_memories_async(to_process))\n",
    "\n",
    "    return response.content\n",
    "\n",
    "# Run conversation as normal\n",
    "response = await chat.ainvoke(\"I like dogs. My dog's name is Fido.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Item(namespace=['memories'], key='b51aff0e-11dd-4988-b94d-7fe49701af8e', value={'kind': 'Memory', 'content': {'content': 'User likes dogs and has a dog named Fido.'}}, created_at='2025-03-05T03:22:06.234598+00:00', updated_at='2025-03-05T03:22:06.234598+00:00', score=None)]\n"
     ]
    }
   ],
   "source": [
    "print(store.search((\"memories\",)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 08:57:57,528 - INFO - Client ID and Client Secret found [azure_openai_llm.py:54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! Cats and dogs can make wonderful companions. Do you have any favorite stories or experiences with your cat or Fido that you'd like to share? Or is there something specific you’d like to know or discuss about them?\n",
      "-----------------------\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.func import entrypoint\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_memory_store_manager\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from azure_openai_llm import get_llm\n",
    "\n",
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "def embed_text(text: str) -> list:\n",
    "    \"\"\"Convert text to embeddings using all-MiniLM-L12-v2.\"\"\"\n",
    "    return embedding_model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "storee = InMemoryStore(index={\"dims\": 384, \"embed\": embed_text})\n",
    "\n",
    "llm = get_llm()\n",
    "\n",
    "# Create memory manager to extract memories from conversations\n",
    "memory_manager = create_memory_store_manager(\n",
    "    llm,\n",
    "    namespace=(\"memories\",),\n",
    ")\n",
    "\n",
    "# Memory extraction queue\n",
    "memory_queue = asyncio.Queue()\n",
    "\n",
    "async def memory_worker():\n",
    "    \"\"\"Background worker that processes memory extraction tasks sequentially inside LangGraph context.\"\"\"\n",
    "    while True:\n",
    "        to_process = await memory_queue.get()\n",
    "        try:\n",
    "            # Use LangGraph's entrypoint to ensure execution within the proper store context\n",
    "            @entrypoint(store=storee)\n",
    "            async def extract_memory(to_process):\n",
    "                await memory_manager.ainvoke(to_process)\n",
    "\n",
    "            await extract_memory.ainvoke(to_process)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Memory extraction error: {e}\")\n",
    "        finally:\n",
    "            memory_queue.task_done()\n",
    "\n",
    "# Start the memory worker in the background\n",
    "asyncio.create_task(memory_worker())\n",
    "\n",
    "@entrypoint(store=storee)\n",
    "async def chat(message: str):\n",
    "    response = llm.invoke(message)\n",
    "\n",
    "    # Prepare conversation history for memory extraction\n",
    "    to_process = {\"messages\": [{\"role\": \"user\", \"content\": message}] + [response]}\n",
    "    \n",
    "    # Add the task to the queue for background processing\n",
    "    await memory_queue.put(to_process)\n",
    "\n",
    "    return response.content\n",
    "\n",
    "# Run conversation as normal\n",
    "response = await chat.ainvoke(\"I like cat. My dog's name is Fido.\")\n",
    "print(response)\n",
    "\n",
    "print('-----------------------')\n",
    "print(storee.search((\"memories\",)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Item(namespace=['memories'], key='77eb67ec-497e-47ff-a533-e9b4ea24a705', value={'kind': 'Memory', 'content': {'content': 'User enjoys cats and has a dog named Fido.'}}, created_at='2025-03-05T03:28:03.998115+00:00', updated_at='2025-03-05T03:28:03.998115+00:00', score=None)]\n"
     ]
    }
   ],
   "source": [
    "print(storee.search((\"memories\",)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 13:13:49,531 - INFO - Client ID and Client Secret found [azure_openai_llm.py:54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemBot: Hi! Ask me anything. (Type 'exit' to stop)\n",
      "MemBot: Hi there! How can I assist you today?\n",
      "\n",
      "--- Stored Memories ---\n",
      "Memory 1: Key=a6fa2045-cb2d-4a84-b071-6b735d5fcdd2, Value=User: hi\n",
      "----------------------\n",
      "\n",
      "MemBot: Hello! What’s on your mind today?\n",
      "\n",
      "--- Stored Memories ---\n",
      "Memory 1: Key=0d82d3d4-5289-46da-a2a3-0be0ea83486b, Value=User: hello | Bot: Hi there! How can I assist you today?\n",
      "Memory 2: Key=a6fa2045-cb2d-4a84-b071-6b735d5fcdd2, Value=User: hi\n",
      "----------------------\n",
      "\n",
      "MemBot: Your first message was \"hi\". Is there anything specific you would like to know or discuss?\n",
      "\n",
      "--- Stored Memories ---\n",
      "Memory 1: Key=0d82d3d4-5289-46da-a2a3-0be0ea83486b, Value=User: hello | Bot: Hi there! How can I assist you today?\n",
      "Memory 2: Key=a6fa2045-cb2d-4a84-b071-6b735d5fcdd2, Value=User: hi\n",
      "----------------------\n",
      "\n",
      "MemBot: Your last question was, \"what did I asked.\" Would you like to ask something else or clarify further?\n",
      "\n",
      "--- Stored Memories ---\n",
      "Memory 1: Key=0d82d3d4-5289-46da-a2a3-0be0ea83486b, Value=User: hello | Bot: Hi there! How can I assist you today?\n",
      "Memory 2: Key=a6fa2045-cb2d-4a84-b071-6b735d5fcdd2, Value=User: hi\n",
      "----------------------\n",
      "\n",
      "MemBot: It seems like your message was empty. If you have something to ask or share, feel free to type it out!\n",
      "\n",
      "--- Stored Memories ---\n",
      "Memory 1: Key=0d82d3d4-5289-46da-a2a3-0be0ea83486b, Value=User: hello | Bot: Hi there! How can I assist you today?\n",
      "Memory 2: Key=a2545b3d-3c47-41af-a4eb-13f8b4b5fd3f, Value=User:  | Bot: Your last question was, 'what did I asked.' Would you like to ask something else or clarify further?\n",
      "Memory 3: Key=a6fa2045-cb2d-4a84-b071-6b735d5fcdd2, Value=User: hi\n",
      "----------------------\n",
      "\n",
      "MemBot: Goodbye!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory storage error: Error code: 429 - {'fault': {'faultstring': 'Spike arrest violation. Allowed rate : MessageRate{messagesPerPeriod=15, periodInMicroseconds=60000000, maxBurstMessageCount=1.5}', 'detail': {'errorcode': 'policies.ratelimit.SpikeArrestViolation'}}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MemBot: A context-aware chatbot using LangGraph and LangMem with InMemorySaver.\n",
    "- Uses Azure ChatGPT for responses.\n",
    "- Stores every query and response in InMemoryStore with all-MiniLM-L12-v2 embeddings.\n",
    "- Persists messages state in-memory via thread_id and InMemorySaver.\n",
    "- Moves memory storage to a background queue for responsiveness.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from azure_openai_llm import get_llm\n",
    "\n",
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "def embed_text(text: str) -> list:\n",
    "    \"\"\"Convert text to embeddings using all-MiniLM-L12-v2.\"\"\"\n",
    "    return embedding_model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "# Memory store and checkpointer setup\n",
    "NAMESPACE = (\"user_1\",)\n",
    "memory_store = InMemoryStore(index={\"dims\": 384, \"embed\": embed_text})\n",
    "checkpointer = MemorySaver()  # In-memory persistence for messages state\n",
    "\n",
    "# Azure ChatGPT model\n",
    "llm = get_llm()\n",
    "\n",
    "# Memory tools\n",
    "manage_memory_tool = create_manage_memory_tool(namespace=NAMESPACE)\n",
    "search_memory_tool = create_search_memory_tool(namespace=NAMESPACE)\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are MemBot, a helpful assistant with memory. Your goals:\n",
    "1. Assist users conversationally.\n",
    "2. Use `manage_memory_tool` to store EVERY user query and assistant response as a single memory entry (e.g., \"User: I like Python | Bot: Noted, you like Python\").\n",
    "3. For questions about past interactions (e.g., \"What was my first message?\"), ALWAYS use `search_memory_tool` to retrieve relevant memories. Sort memories by order (earliest first) and return the EXACT user input from the first relevant memory. If the tool fails, use the conversation history (messages) to find the first user input.\n",
    "Keep responses natural and use the full conversation history (passed in messages) for coherence.\n",
    "\"\"\"\n",
    "\n",
    "# Agent setup\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[manage_memory_tool, search_memory_tool],\n",
    "    store=memory_store,\n",
    "    checkpointer=checkpointer,\n",
    "    prompt=SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "# Queue for background memory storage\n",
    "memory_queue = asyncio.Queue()\n",
    "\n",
    "async def memory_worker():\n",
    "    \"\"\"Background worker that processes memory storage asynchronously.\"\"\"\n",
    "    while True:\n",
    "        memory_entry = await memory_queue.get()\n",
    "        try:\n",
    "            agent.invoke(\n",
    "                {\"messages\": [{\"role\": \"system\", \"content\": f\"Use manage_memory_tool to store: {memory_entry}\"}]},\n",
    "                config={\"configurable\": {\"thread_id\": \"user_1_thread\"}}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Memory storage error: {e}\")\n",
    "        finally:\n",
    "            memory_queue.task_done()\n",
    "\n",
    "# Start background memory processing\n",
    "asyncio.create_task(memory_worker())\n",
    "\n",
    "def print_stored_memories() -> None:\n",
    "    \"\"\"Print all memories stored in InMemoryStore.\"\"\"\n",
    "    print(\"\\n--- Stored Memories ---\")\n",
    "    try:\n",
    "        all_memories = memory_store._data.get(NAMESPACE, {})\n",
    "        if not all_memories:\n",
    "            print(\"No memories stored yet.\")\n",
    "        else:\n",
    "            for i, (key, item) in enumerate(sorted(all_memories.items(), key=lambda x: x[0]), 1):\n",
    "                value = getattr(item, \"value\", \"None\") if item else \"None\"\n",
    "                if isinstance(value, dict) and \"content\" in value:\n",
    "                    value = value[\"content\"]\n",
    "                print(f\"Memory {i}: Key={key}, Value={value}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving memories: {e}\")\n",
    "    print(\"----------------------\\n\")\n",
    "\n",
    "def chat_with_membot() -> None:\n",
    "    \"\"\"Run an interactive chat loop with MemBot.\"\"\"\n",
    "    print(\"MemBot: Hi! Ask me anything. (Type 'exit' to stop)\")\n",
    "    conversation_history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    config = {\"configurable\": {\"thread_id\": \"user_1_thread\"}}  # Thread-specific state\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"MemBot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # Normalize input\n",
    "            normalized_input = user_input.lower()\n",
    "\n",
    "            # Add user input to history\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "            # Invoke agent with thread config\n",
    "            response = agent.invoke({\"messages\": conversation_history}, config=config)\n",
    "\n",
    "            # Extract response\n",
    "            ai_response = (\n",
    "                response[\"messages\"][-1].content\n",
    "                if isinstance(response, dict) and \"messages\" in response\n",
    "                else str(response)\n",
    "            )\n",
    "            print(f\"MemBot: {ai_response}\")\n",
    "\n",
    "            # Add bot response to history\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "            # Store memory asynchronously in the background\n",
    "            memory_entry = f\"User: {normalized_input} | Bot: {ai_response}\"\n",
    "            asyncio.create_task(memory_queue.put(memory_entry))  # Queue it\n",
    "\n",
    "            # Show stored memories\n",
    "            print_stored_memories()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nMemBot: Goodbye!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_membot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
